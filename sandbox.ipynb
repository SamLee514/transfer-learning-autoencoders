{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='gray')\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Activation,\n",
    "                                    Flatten, Dense, Reshape, Dropout, Add, Input)\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from random import randint, uniform\n",
    "from numpy.random import normal, uniform\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.draw import ellipse_perimeter, disk\n",
    "from scipy.interpolate import interp1d \n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model defs and pipeline processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make this more OOP-like. Also add uncertainty option for mobilenet version\n",
    "class AEUnet:\n",
    "    @staticmethod\n",
    "    def collin(dim=256,depth=1,filters=(16,32,64,128,256),latentDepth=512,MCDropout=False):\n",
    "        '''\n",
    "        dim: dim of square input image\n",
    "        depth: number of channels in input image\n",
    "        filters: tuple for set of convolution filters, defaulted to paper implementation.\n",
    "        latentDepth: depth of latent layer\n",
    "        '''\n",
    "        inputShape=(dim,dim,depth)\n",
    "        chanDim=-1 #channel dimension (-1) applies batch norm for each layer (or depth) Unknowns=[mu(untrainable),sigma(untrainable),gamma,beta]*depth\n",
    "\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "\n",
    "        # generate:conv=>relu=>bn layers\n",
    "        # additionally, save skip connections and add dropout\n",
    "        # Dropout layers increase by 0.1 as the encoder gets deeper\n",
    "        skips = []\n",
    "        for i, f in enumerate(filters):\n",
    "            x = Conv2D(f,(5,5),strides=2,padding='same')(x)\n",
    "            x1 = [x]\n",
    "            skips.append(x1[0])\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x) \n",
    "            x = Dropout(0.1 * i)(x, training=MCDropout)\n",
    "\n",
    "        # For the UNet build, use a conv layer instead of dense for latent\n",
    "        latent = Conv2D(latentDepth, (5, 5), strides=2, padding='same', name='latent')(x)\n",
    "        y = latent\n",
    "        \n",
    "        #decode the latent space, incorporating skip layers\n",
    "        for i, f in reversed(list(enumerate(filters))):\n",
    "            y = Conv2DTranspose(f,(5,5),strides=2,padding='same')(y)\n",
    "            y = Add()([skips[i], y])\n",
    "            y = LeakyReLU(alpha=0.2)(y)\n",
    "            y = BatchNormalization(axis=chanDim)(y)\n",
    "\n",
    "        \n",
    "        # Technically not strictly following the paper, this should be a straight-up upsampling\n",
    "        y = Conv2DTranspose(depth,(5,5),strides=2,padding='same')(y)\n",
    "        outputs = Activation('sigmoid')(y)\n",
    "        autoencoder = Model(inputs,outputs,name='autoencoder')\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "    # This one flips the order of dropout and batchnorm and adds more dropout to decoder.\n",
    "    def collin_experimental(dim=256,depth=1,filters=(16,32,64,128,256),latentDepth=512,MCDropout=False):\n",
    "        '''\n",
    "        dim: dim of square input image\n",
    "        depth: number of channels in input image\n",
    "        filters: tuple for set of convolution filters, defaulted to paper implementation.\n",
    "        latentDepth: depth of latent layer\n",
    "        '''\n",
    "        inputShape=(dim,dim,depth)\n",
    "        chanDim=-1 #channel dimension (-1) applies batch norm for each layer (or depth) Unknowns=[mu(untrainable),sigma(untrainable),gamma,beta]*depth\n",
    "\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "\n",
    "        # generate:conv=>relu=>bn layers\n",
    "        # additionally, save skip connections and add dropout\n",
    "        # Dropout layers increase by 0.1 as the encoder gets deeper\n",
    "        skips = []\n",
    "        for i, f in enumerate(filters):\n",
    "            x = Conv2D(f,(5,5),strides=2,padding='same')(x)\n",
    "            x1 = [x]\n",
    "            skips.append(x1[0])\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = Dropout(0.1 * i)(x, training=MCDropout)\n",
    "            x = BatchNormalization(axis=chanDim)(x) \n",
    "\n",
    "        # For the UNet build, use a conv layer instead of dense for latent\n",
    "        latent = Conv2D(latentDepth, (5, 5), strides=2, padding='same', name='latent')(x)\n",
    "        y = latent\n",
    "        \n",
    "        #decode the latent space, incorporating skip layers\n",
    "        for i, f in reversed(list(enumerate(filters))):\n",
    "            y = Conv2DTranspose(f,(5,5),strides=2,padding='same')(y)\n",
    "            y = Add()([skips[i], y])\n",
    "            y = LeakyReLU(alpha=0.2)(y)\n",
    "            x = Dropout(0.1 * (4-i))(x, training=MCDropout)\n",
    "            y = BatchNormalization(axis=chanDim)(y)\n",
    "\n",
    "        \n",
    "        # Technically not strictly following the paper, this should be a straight-up upsampling\n",
    "        y = Conv2DTranspose(depth,(5,5),strides=2,padding='same')(y)\n",
    "        outputs = Activation('sigmoid')(y)\n",
    "        autoencoder = Model(inputs,outputs,name='autoencoder')\n",
    "\n",
    "        return autoencoder\n",
    "\n",
    "    def mobilenet(dim, depth, filters, latentDepth, output_channels=3,trainable=True, MCDropout=False):\n",
    "        \n",
    "        # not actually using filters option. Just had it to satisfy the system.\n",
    "        \n",
    "        base_model = tf.keras.applications.MobileNetV2(input_shape=[dim, dim, 3], include_top=False)\n",
    "\n",
    "        # Use the activations of these layers\n",
    "        layer_names = [\n",
    "            'block_1_expand_relu',   # 64x64\n",
    "            'block_3_expand_relu',   # 32x32\n",
    "            'block_6_expand_relu',   # 16x16\n",
    "            'block_13_expand_relu',  # 8x8\n",
    "            'block_16_project',      # 4x4\n",
    "        ]\n",
    "        layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "        # Create the feature extraction model\n",
    "        down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "        down_stack.trainable = trainable # From TLP experience, this should be True\n",
    "\n",
    "        # \"Add\" layer version\n",
    "#         up_stack = [\n",
    "#             pix2pix.upsample(576, 3),  # 4x4 -> 8x8\n",
    "#             pix2pix.upsample(192, 3),  # 8x8 -> 16x16\n",
    "#             pix2pix.upsample(144, 3),  # 16x16 -> 32x32\n",
    "#             pix2pix.upsample(96, 3),   # 32x32 -> 64x64\n",
    "#         ]\n",
    "\n",
    "        # \"Concat\" layer version\n",
    "        up_stack = [\n",
    "            pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "            pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "            pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "            pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "        ]\n",
    "\n",
    "        inputs = tf.keras.layers.Input(shape=[dim, dim, 3])\n",
    "        x = inputs\n",
    "\n",
    "        # Downsampling through the model\n",
    "        skips = down_stack(x)\n",
    "        x = skips[-1]\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Latent space\n",
    "        x = Dense(latentDepth)(x)\n",
    "        \n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            concat = tf.keras.layers.Concatenate()\n",
    "            x = concat([x, skip])\n",
    "            # Add dropout mostly just for uncertainty functionality\n",
    "            x = Dropout(0.1 * (5-i))(x, training=MCDropout)\n",
    "\n",
    "        # This is the last layer of the model\n",
    "        last = tf.keras.layers.Conv2DTranspose(\n",
    "            output_channels, 3, strides=2,\n",
    "            padding='same')(x)  #64x64 -> 128x128\n",
    "\n",
    "        # Modification to make autoencoder\n",
    "        outputs = Activation('sigmoid')(last)\n",
    "        autoencoder = Model(inputs,outputs,name='autoencoder')\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! If blur is too high (0.08), then it won't get real defects good\n",
    "def add_stain(img,\n",
    "              min_size=10,\n",
    "              max_size=40,\n",
    "              min_color=0.,\n",
    "              max_color=255.,\n",
    "              cx_range=None,\n",
    "              cy_range=None,\n",
    "              irregularity=0.02,\n",
    "              blur=0.02):\n",
    "    '''\n",
    "    Draw an ellipse-like shape \n",
    "    INPUT : \n",
    "        - img: image to corrupt with an elliptical stain\n",
    "        - min_size, max_size: bounds for stain size in percentage\n",
    "        - min_color, max_color: bounds of intensities to sample from (0 - 255)\n",
    "        - cx_range, cy_range: (optional) tuples to constrain location (in percentage of image) of stain center\n",
    "        - irregularity: level of irregularity of the ellipse (0: no)\n",
    "        - blur: blur edges (0: no)\n",
    "    OUTPUT: \n",
    "        - corrupted image\n",
    "    '''\n",
    "    \n",
    "    assert min_size <= max_size, 'min_size must not be greater than max_size'\n",
    "    assert min_size >= 0 and max_size <= 100, 'invalid sizes. Must be in range [0, 100]'\n",
    "    assert min_color <= max_color, 'min_color must not be greater than max_color'\n",
    "    assert max_color >= 0 and max_color <= 255, 'invalid colors, Must be in range[0, 255]'\n",
    "\n",
    "    color    = randint(min_color, max_color)\n",
    "    col, row = img.shape[1], img.shape[0]\n",
    "    rotation = uniform(0, 2*np.pi)\n",
    "    ax_x = int(randint(min_size, max_size)/2 / 100*col)\n",
    "    ax_y = int(randint(min_size, max_size)/2 / 100*row)\n",
    "    \n",
    "    if cx_range is not None:\n",
    "        cx_lower = int(cx_range[0] / 100.*col)\n",
    "        cx_upper = int(cx_range[1] / 100.*col)\n",
    "    else:\n",
    "        cx_lower = int(max_size/2 / 100.*col)\n",
    "        cx_upper = int((100 - max_size/2) / 100.*col)\n",
    "        \n",
    "    if cy_range is not None:\n",
    "        cy_lower = int(cy_range[0] / 100.*row)\n",
    "        cy_upper = int(cy_range[1] / 100.*row)\n",
    "    else:\n",
    "        cy_lower = int(max_size/2 / 100.*row)\n",
    "        cy_upper = int((100 - max_size/2) / 100.*row)\n",
    "    \n",
    "    cx, cy = randint(cx_lower, cx_upper), randint(cy_lower, cy_upper)\n",
    "    y,x      = ellipse_perimeter(cy, cx, ax_y, ax_x, rotation)\n",
    "    # Flip x and y because opencv is annoying\n",
    "    contour  = np.array([[i,j] for i,j in zip(x,y)])\n",
    "    # Change the shape of the ellipse \n",
    "    if irregularity > 0: \n",
    "        contour = _perturbate_ellipse(contour, cx, cy, (ax_x+ax_y)/2, irregularity)\n",
    "\n",
    "    mask = np.zeros((row, col)) \n",
    "    mask = cv2.drawContours(mask, [contour], -1, 1, -1)\n",
    "\n",
    "    if blur != 0 : \n",
    "        mask = gaussian_filter(mask, max(ax_x,ax_y)*blur)\n",
    "\n",
    "    rgb_mask     = np.dstack([mask]*3)\n",
    "    not_modified = np.subtract(np.ones(img.shape), rgb_mask)\n",
    "    stain        = 255*random_noise(np.zeros(img.shape), mode='gaussian', mean = color/255., var = 0.05/255.)\n",
    "    result       = np.add( np.multiply(img,not_modified), np.multiply(stain,rgb_mask) ) \n",
    "\n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "'''\n",
    "Helper functions for stain\n",
    "'''\n",
    "def _perturbate_ellipse(contour, cx, cy, diag, irregularity):\n",
    "    # Keep only some points\n",
    "    if len(contour) < 20: \n",
    "        pts = contour\n",
    "    else: \n",
    "        pts = contour[0::int(len(contour)/20)]\n",
    "\n",
    "    # Perturbate coordinates\n",
    "    for idx,pt in enumerate(pts): \n",
    "        pts[idx] = [pt[0]+randint(-int(diag*irregularity), int(diag*irregularity)),\n",
    "                    pt[1]+randint(-int(diag*irregularity),int(diag*irregularity))]\n",
    "    pts = sorted(pts, key=lambda p: _clockwiseangle(p, cx, cy))\n",
    "    pts.append([pts[0][0], pts[0][1]])\n",
    "\n",
    "    # Interpolate between remaining points\n",
    "    i = np.arange(len(pts))\n",
    "    interp_i = np.linspace(0, i.max(), 10 * i.max())\n",
    "    yi = interp1d(i, np.array(pts)[:,0], kind='cubic')(interp_i)\n",
    "    xi = interp1d(i, np.array(pts)[:,1], kind='cubic')(interp_i) \n",
    " \n",
    "    return np.array([[int(i),int(j)] for i,j in zip(yi,xi)])\n",
    "\n",
    "def _clockwiseangle(point, cx, cy):\n",
    "    refvec = [0 , 1]\n",
    "    vector = [point[0]-cy, point[1]-cx]\n",
    "    norm   = math.hypot(vector[0], vector[1])\n",
    "    # If length is zero there is no angle\n",
    "    if norm == 0:\n",
    "        return -math.pi\n",
    "    normalized = [vector[0]/norm, vector[1]/norm]\n",
    "    dotprod    = normalized[0]*refvec[0] + normalized[1]*refvec[1] \n",
    "    diffprod   = refvec[1]*normalized[0] - refvec[0]*normalized[1] \n",
    "    angle      = math.atan2(diffprod, dotprod)\n",
    "    if angle < 0:\n",
    "        return 2*math.pi+angle\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(object_path, autoencoder, method=None, model_function=None):\n",
    "    \n",
    "    ground_path  = os.path.join(object_path, 'ground_truth')\n",
    "    images_path  = os.path.join(object_path, 'test')\n",
    "    defect_types = os.listdir(ground_path)\n",
    "\n",
    "    if method == 'uncertainty':\n",
    "        print('[INFO]: Generating uncertainty model')\n",
    "        assert model_function is not None, 'Must provide function that autoencoder was created with if using uncertainty method'\n",
    "        weights = autoencoder.get_weights()\n",
    "        uncertainty_ae = model_function(MCDropout=True)\n",
    "        uncertainty_ae.set_weights(weights)\n",
    "        \n",
    "    # Calculate the overall AUC separately. Averaging the AUC's for individual categories gives\n",
    "    # a slightly higher score.\n",
    "    all_ground  = []\n",
    "    all_results = []\n",
    "    for defect_type in defect_types:\n",
    "        ground_vector = []\n",
    "        input_images  = []\n",
    "        masks  = sorted(os.listdir(os.path.join(ground_path, defect_type)))\n",
    "        images = sorted(os.listdir(os.path.join(images_path, defect_type)))\n",
    "        for mask in masks:\n",
    "            full_path = os.path.join(ground_path, defect_type, mask)\n",
    "            ground = cv2.imread(full_path)[...,0]\n",
    "            ground = cv2.resize(ground, (DIM, DIM))\n",
    "            ground[ground > 0] = 1\n",
    "            ground_vector.append(ground)\n",
    "        for image in images:\n",
    "            full_path = os.path.join(images_path, defect_type, image)\n",
    "            image = cv2.imread(full_path)\n",
    "            if DEPTH == 1:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (DIM, DIM)).astype('float32') / 255\n",
    "            input_images.append(image)\n",
    "\n",
    "        # Predict images\n",
    "        input_images = np.array(input_images)\n",
    "        if method =='uncertainty':\n",
    "            results_list = []\n",
    "            # Using 30 runs according to the paper\n",
    "            for i in range(30):\n",
    "                results = uncertainty_ae(input_images).numpy()\n",
    "                results = np.squeeze(results, axis=-1)\n",
    "                results_list.append(results)\n",
    "            results_vector = np.std(results_list, axis=0)\n",
    "            plt.imshow(results_vector[0])\n",
    "            plt.show()\n",
    "            results_vector = results_vector.flatten()\n",
    "        else:\n",
    "            results_vector = autoencoder(input_images).numpy()\n",
    "            results_vector = np.squeeze(results_vector, axis=-1)\n",
    "            results_vector = np.abs(results_vector - input_images).flatten()\n",
    "        ground_vector = np.array(ground_vector).flatten()\n",
    "        score = roc_auc_score(ground_vector, results_vector)\n",
    "        print(f'AUC for {defect_type}:', score)\n",
    "\n",
    "        for i in ground_vector: all_ground.append(i)\n",
    "        for i in results_vector: all_results.append(i)\n",
    "\n",
    "    full_score = roc_auc_score(all_ground, all_results)\n",
    "    print('Average AUC score:', full_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config cells.\n",
    "\n",
    "Ideally these should be the only things run differently across different runs but things happen and these aren't exactly backwards compatible lol\n",
    "\n",
    "\n",
    "### Experiment Log\n",
    "- I'm not gonna document everything (writing at 1/7/21) before this, too lazy\n",
    "- Mobilenet with 224 and trainable is bad. Starts overfitting really quick.\n",
    "- 1/7/21 See what happens if we add a dense latent space to the mobilenet Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 1000\n",
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "LOSS = 'mse'\n",
    "testImg = 'data/mvtec_anomaly_detection/carpet/test/hole/000.png'\n",
    "modelSavePath = 'outputs/prototype.pb'\n",
    "historySavePath = 'outputs/prototype_history'\n",
    "def create_autoencoder():\n",
    "    return AEUnet.mobilenet(3, DIM)\n",
    "STAIN_PROPS = [10, 40, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as Prototype but for only 100 epochs, as 50 was where things were really good in the last\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "LOSS = 'mse'\n",
    "testImg = 'data/mvtec_anomaly_detection/carpet/test/hole/000.png'\n",
    "modelSavePath = 'outputs/prototype1.pb'\n",
    "historySavePath = 'outputs/prototype1_history'\n",
    "def create_autoencoder():\n",
    "    return AEUnet.mobilenet(3, DIM)\n",
    "STAIN_PROPS = [10, 40, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as Prototype but for only 100 epochs, and less aggro stain colors\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "LOSS = 'mse'\n",
    "testImg = 'data/mvtec_anomaly_detection/carpet/test/hole/000.png'\n",
    "modelSavePath = 'outputs/prototype2.pb'\n",
    "historySavePath = 'outputs/prototype1_history'\n",
    "def create_autoencoder():\n",
    "    return AEUnet.mobilenet(3, DIM)\n",
    "STAIN_PROPS = [10, 40, 150, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin carpet\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "DEPTH = 1\n",
    "FILTERS = (DEPTH * 16, DEPTH * 32, DEPTH * 64, DEPTH * 128, DEPTH * 256)\n",
    "LATENT = DEPTH * 512\n",
    "EPOCHS = 300\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/carpet/test/cut/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/hole/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/color/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_carpet.pb'\n",
    "historySavePath = 'outputs/collin_history'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Resid 80.4\n",
    "'''\n",
    "AUC for metal_contamination: 0.7976807709851567\n",
    "AUC for cut: 0.7676479321130159\n",
    "AUC for thread: 0.7704211380532168\n",
    "AUC for color: 0.9335107927634231\n",
    "AUC for hole: 0.7793682318671236\n",
    "Average AUC score: 0.8040516566229404\n",
    "'''\n",
    "\n",
    "# Uncert 94.1\n",
    "'''\n",
    "AUC for metal_contamination: 0.9754365996420657\n",
    "AUC for cut: 0.9370223507151894\n",
    "AUC for thread: 0.8651138925990813\n",
    "AUC for color: 0.9664916496937275\n",
    "AUC for hole: 0.9529737797856629\n",
    "Average AUC score: 0.9408986430342042\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin carpet 3D (12/18)\n",
    "DIM = 256\n",
    "DEPTH = 3\n",
    "FILTERS = (DEPTH * 16, DEPTH * 32, DEPTH * 64, DEPTH * 128, DEPTH * 256)\n",
    "LATENT = DEPTH * 512\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 300\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/carpet/test/cut/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/hole/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/color/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_carpet_3D.pb'\n",
    "historySavePath = 'outputs/collin_history_3D'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Resid: 86.3\n",
    "'''\n",
    "AUC for metal_contamination: 0.9434638215965654\n",
    "AUC for cut: 0.830731196021751\n",
    "AUC for thread: 0.8622653518916192\n",
    "AUC for color: 0.909399036895851\n",
    "AUC for hole: 0.8564032503952155\n",
    "Average AUC score: 0.8628954155629363\n",
    "'''\n",
    "'''\n",
    "Uncert: 95.6\n",
    "AUC for metal_contamination: 0.9878566653632854\n",
    "AUC for cut: 0.9524887847802426\n",
    "AUC for thread: 0.9442395637140933\n",
    "AUC for color: 0.947311419083156\n",
    "AUC for hole: 0.9645082518587725\n",
    "Average AUC score: 0.955757885451878\n",
    "\n",
    "\n",
    "Interestingly, for both resid and uncert, the 1D version outperforms the 3D version.\n",
    "PROBABLY because the color stains are still grayscale even for the color version.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin carpet 3D half (12/18)\n",
    "DIM = 256\n",
    "DEPTH = 3\n",
    "FILTERS = (16, 32, 64, 128, 256)\n",
    "LATENT = 512\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 300\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/carpet/test/cut/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/hole/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/color/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_carpet_3D_half.pb'\n",
    "historySavePath = 'outputs/collin_history_3D_half'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "'''\n",
    "RESIDUAL:\n",
    "AUC for metal_contamination: 0.8750591164360945\n",
    "AUC for cut: 0.7556173948053688\n",
    "AUC for thread: 0.8248140746826649\n",
    "AUC for color: 0.9337140297783294\n",
    "AUC for hole: 0.7727851246013533\n",
    "Average AUC score: 0.8087412669619461\n",
    "\n",
    "UNCERT\n",
    "AUC for metal_contamination: 0.9912070449011537\n",
    "AUC for cut: 0.9317350012144429\n",
    "AUC for thread: 0.9164020856059844\n",
    "AUC for color: 0.9660207290746308\n",
    "AUC for hole: 0.9474724475126308\n",
    "Average AUC score: 0.943171346244713\n",
    "\n",
    "So it looks like if you go 3D without increasing the depth of the model then you only get marginal improvements.\n",
    "However, full 3D with proportional depth is so freaking slow.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "base_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobilenet carpet (12/18)\n",
    "DIM = 256\n",
    "DEPTH = 3\n",
    "FILTERS = (16, 32, 64, 128, 256)\n",
    "LATENT = 512\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 300\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/carpet/test/cut/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/hole/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/color/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/good/000.png'\n",
    "]\n",
    "modelSavePath   = 'outputs/mobilenet_carpet.pb'\n",
    "historySavePath = 'outputs/mobilenet_history'\n",
    "modelSavePath2  = 'outputs/mobilenet_carpet_full.pb/'\n",
    "historySavePath = 'outputs/mobilenet_history_full'\n",
    "create_autoencoder = AEUnet.mobilenet\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# will require some manual changes downstream. Need to run second training run unfrozen.\n",
    "\n",
    "# Before unfreezing:\n",
    "'''\n",
    "Resid:\n",
    "AUC for metal_contamination: 0.7127854998468568\n",
    "AUC for cut: 0.6054001665842494\n",
    "AUC for thread: 0.7518795370620136\n",
    "AUC for color: 0.8750955313057392\n",
    "AUC for hole: 0.6014542457150654\n",
    "Average AUC score: 0.6794173591587408\n",
    "\n",
    "Uncert:\n",
    "AUC for metal_contamination: 0.5112918904603708\n",
    "AUC for cut: 0.5037951189852011\n",
    "AUC for thread: 0.5143294279317485\n",
    "AUC for color: 0.43974230007975773\n",
    "AUC for hole: 0.49555054327457027\n",
    "Average AUC score: 0.4917689652321947\n",
    "\n",
    "So maybe flip the order of dropout for future?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mobilenet carpet 2 (1/7/21)\n",
    "DIM = 224\n",
    "DEPTH = 3\n",
    "FILTERS = (16, 32, 64, 128, 256)\n",
    "LATENT = 512\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 300\n",
    "INIT_LR = 1e-4\n",
    "BS = 32\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/carpet/test/cut/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/hole/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/color/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/good/000.png'\n",
    "]\n",
    "modelSavePath   = 'outputs/mobilenet_carpet_native_res.pb'\n",
    "historySavePath = 'outputs/mobilenet_history_native_res'\n",
    "create_autoencoder = AEUnet.mobilenet\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "TRAINABLE=True\n",
    "LR_SCHEDULE_TYPE = 'cosine'\n",
    "\n",
    "# So I haven't finished training this one but from the looks of it, changing the starting LR from 1e-3 to 1e-4 \n",
    "# allowed the loss to get lower, so perhaps this is really just a learning rate problem.\n",
    "# Also the latent space bit doesn't seem to help. Might as well get rid of it.\n",
    "\n",
    "# Things to try next:\n",
    "    # Freeze the layers and train the decoder first. Then unfreeze and train both at an extremely low learning rate.\n",
    "    # If that doesn't work, go back to full unfreeze and try 1e-4 start with exponential decay\n",
    "    # If that doesn't work, go back to cosine decay and straight up start with like 1e-5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_autoencoder(depth=DEPTH, filters=FILTERS, latentDepth=LATENT)\n",
    "model.save(modelSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin carpet experimental (12/18)\n",
    "DIM = 256\n",
    "DEPTH = 1\n",
    "FILTERS = (16, 32, 64, 128, 256)\n",
    "LATENT = 512\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 300\n",
    "INIT_LR = 5e-4\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/carpet/test/cut/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/hole/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/color/000.png',\n",
    "    'data/mvtec_anomaly_detection/carpet/test/good/000.png'\n",
    "]\n",
    "modelSavePath   = 'outputs/collin_carpet_experimental.pb'\n",
    "historySavePath = 'outputs/collin_carpet_experimental_history'\n",
    "create_autoencoder = AEUnet.collin_experimental\n",
    "STAIN_PROPS = [10, 40, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin bottle\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/bottle/train/good/'\n",
    "EPOCHS = 300\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_large/000.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_large/001.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_small/000.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_small/001.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_bottle.pb'\n",
    "historySavePath = 'outputs/collin_history/bottle'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin toothbrush\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/toothbrush/train/good/'\n",
    "EPOCHS = 1400\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/000.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/001.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/002.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/003.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/004.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/005.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_toothbrush.pb'\n",
    "historySavePath = 'outputs/collin_history/toothbrush'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Achieves 91.1% for both. However, every reconstruction has a noticeable blowout in the top of the brush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin toothbrush 2 (12/17)\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/toothbrush/train/good/'\n",
    "EPOCHS = 1400\n",
    "INIT_LR = 1e-4\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/000.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/001.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/002.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/003.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/004.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/005.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_toothbrush_2.pb'\n",
    "historySavePath = 'outputs/collin_history/toothbrush_2'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Quit this one early as there was significant blowout and validation loss bottomed out at around 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin toothbrush 3 (12/17)\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/toothbrush/train/good/'\n",
    "EPOCHS = 1400\n",
    "INIT_LR = 1e-2\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/000.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/001.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/002.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/003.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/004.png',\n",
    "    'data/mvtec_anomaly_detection/toothbrush/test/defective/005.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_toothbrush_3.pb'\n",
    "historySavePath = 'outputs/collin_history/toothbrush_3'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Achieves 91.0% and 94.0% for resid/uncert. No more blowout which is nice, though resid went down from original\n",
    "# somehow...\n",
    "\n",
    "# Suggests that key to solving blowout is to increase LR? Might also help get to lower loss. Test on bottle.\n",
    "# I should record the lowest loss and when that happens lol\n",
    "\n",
    "# The one for this is like 2.39e-4 for valid loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin bottle 2 (12/17)\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/bottle/train/good/'\n",
    "EPOCHS = 400\n",
    "INIT_LR = 1e-2\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_large/000.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_large/001.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_small/000.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/broken_small/001.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/bottle/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_bottle_2.pb'\n",
    "historySavePath = 'outputs/collin_history_2/bottle'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Resid: 85.9, Uncert: 87.0%\n",
    "# val loss 2.34e-4 at epoch 389/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin grid (12/17)\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/grid/train/good/'\n",
    "EPOCHS = 320\n",
    "INIT_LR = 1e-2\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/grid/test/bent/000.png',\n",
    "    'data/mvtec_anomaly_detection/grid/test/broken/000.png',\n",
    "    'data/mvtec_anomaly_detection/grid/test/glue/000.png',\n",
    "    'data/mvtec_anomaly_detection/grid/test/metal_contamination/000.png',\n",
    "    'data/mvtec_anomaly_detection/grid/test/thread/000.png',\n",
    "    'data/mvtec_anomaly_detection/grid/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_grid.pb'\n",
    "historySavePath = 'outputs/collin_history/grid'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Resid: 88.8, Uncert: 93.9%\n",
    "# val loss 2.6010e-04 at 303/320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collin capsule (12/17)\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/capsule/train/good/'\n",
    "EPOCHS = 310\n",
    "INIT_LR = 1e-2\n",
    "BS = 16\n",
    "LOSS = 'mse'\n",
    "testImgs = [\n",
    "    'data/mvtec_anomaly_detection/capsule/test/crack/000.png',\n",
    "    'data/mvtec_anomaly_detection/capsule/test/faulty_imprint/000.png',\n",
    "    'data/mvtec_anomaly_detection/capsule/test/poke/000.png',\n",
    "    'data/mvtec_anomaly_detection/capsule/test/scratch/000.png',\n",
    "    'data/mvtec_anomaly_detection/capsule/test/squeeze/000.png',\n",
    "    'data/mvtec_anomaly_detection/capsule/test/good/000.png'\n",
    "]\n",
    "modelSavePath = 'outputs/collin_capsule.pb'\n",
    "historySavePath = 'outputs/collin_history/capsule'\n",
    "create_autoencoder = AEUnet.collin\n",
    "STAIN_PROPS = [10, 40, 0, 255]\n",
    "\n",
    "# Resid: 77.3, Uncert: 93.3\n",
    "# Best val loss: 0.00013910970301367342 at epoch 298\n",
    "\n",
    "# Likely can improve performance by localizing stains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input pipeline and images to do intermittent eyeballing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data input pipeline\n",
    "def stain_wrapper(image, target):\n",
    "    image = image.numpy()\n",
    "    target = target.numpy()\n",
    "    image = add_stain(image, STAIN_PROPS[0], STAIN_PROPS[1], STAIN_PROPS[2], STAIN_PROPS[3])\n",
    "    if DEPTH == 1:\n",
    "        image = np.expand_dims(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), axis=-1)\n",
    "        target = np.expand_dims(cv2.cvtColor(target, cv2.COLOR_BGR2GRAY), axis=-1)\n",
    "    return image.astype('float32') / 255, target.astype('float32') / 255\n",
    "\n",
    "stain_lambda = lambda x, y: tf.py_function(stain_wrapper, [x, y], ['float32', 'float32'])\n",
    "\n",
    "imgDir = os.listdir(DATA_DIR)\n",
    "print(f'[INFO] Reading {len(imgDir)} images into tensors...')\n",
    "imgs = np.empty((len(imgDir), DIM, DIM, 3), dtype='uint8')\n",
    "for i, imgName in enumerate(imgDir):\n",
    "    img = cv2.imread(os.path.join(DATA_DIR, imgName))\n",
    "    img = cv2.resize(img, (DIM, DIM))\n",
    "    imgs[i] = img\n",
    "print(f'[SANITY CHECK] image shape: {imgs[0].shape}')\n",
    "plt.imshow(imgs[0])\n",
    "plt.show()\n",
    "    \n",
    "print('[INFO] Creating TF Datasets...')\n",
    "(trainX, valX)=train_test_split(imgs,test_size=0.2,random_state=42)\n",
    "\n",
    "trainData = tf.data.Dataset.from_tensor_slices((trainX, trainX))\\\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
    "            .map(stain_lambda)\\\n",
    "            .shuffle(trainX.shape[0])\\\n",
    "            .batch(BS)\n",
    "            \n",
    "valData   = tf.data.Dataset.from_tensor_slices((valX, valX))\\\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
    "            .map(stain_lambda)\\\n",
    "            .batch(BS)\n",
    "            \n",
    "print('[INFO] Creating eyeballing set...')\n",
    "visImages = np.empty((6, DIM, DIM, DEPTH))\n",
    "for i, path in enumerate(testImgs):\n",
    "    img = cv2.imread(path)\n",
    "    # By convention (my convention), last image should be the artificially stained one\n",
    "    if i == 5:\n",
    "        img = add_stain(img, STAIN_PROPS[0], STAIN_PROPS[1], STAIN_PROPS[2], STAIN_PROPS[3])\n",
    "    img = cv2.resize(img, (DIM, DIM))\n",
    "    if DEPTH == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.expand_dims(img, axis=[0, -1])\n",
    "    visImages[i] = img.astype('float32') / 255\n",
    "    plt.imshow(visImages[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Try After Baseline\n",
    "\n",
    "### First\n",
    "- Benchmark AMUnet\n",
    "\n",
    "REMEMBER to take all the optimizations you used for the mobilenet version and reapply them to base Collin and 3D Collin for fairness. Necessary optimizations include localized staining and 3D colorings.\n",
    "\n",
    "### Easy\n",
    "- Adabound optimizer\n",
    "- More aggro synthetic defect suite\n",
    "- SSIM loss\n",
    "- Uncertainty vs Residual\n",
    "    - Are you serious. Uncertainty is literally just running the model 30 times with the dropout layers active.\n",
    "    - There is a workaround here https://github.com/keras-team/keras/issues/9412#issuecomment-366487249 but I don't understand it and think I tried it already? Looks like (from the lower comments) people have had trouble with it in new TF. Here is my proposed workflow: Define a new model with MC dropout (aka train=True) and port over the weights.\n",
    "\n",
    "### Research\n",
    "- Energy method\n",
    "\n",
    "### More Benchmarking\n",
    "- VAE\n",
    "- L2 Autoencoder\n",
    "- Try concat layers instead of adding.\n",
    "- Other UNet models\n",
    "    - See https://github.com/qubvel/segmentation_models/blob/master/segmentation_models/models/unet.py\n",
    "- Other segmentation-based\n",
    "\n",
    "- Might be worthwhile to test how it performs when the area of staining is limited to the object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_SCHEDULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras_adabound import AdaBound\n",
    "\n",
    "print('[INFO] building autoencoder...')\n",
    "K.clear_session()\n",
    "autoencoder = create_autoencoder(dim=DIM, depth=DEPTH, filters=FILTERS, latentDepth=LATENT, trainable=TRAINABLE)\n",
    "# Check if it's there\n",
    "try:\n",
    "    if LR_SCHEDULE == 'exponential':\n",
    "        print('Using exponential decay')\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=INIT_LR,\n",
    "            decay_steps=EPOCHS * trainX.shape[0] // BS,\n",
    "            decay_rate=0.96,\n",
    "            staircase=True)\n",
    "    elif LR_SCHEDULE == 'cosine':\n",
    "        print('Using cosine decay')\n",
    "        lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "            initial_learning_rate=INIT_LR,\n",
    "            decay_steps=EPOCHS * trainX.shape[0] // BS)\n",
    "    else:\n",
    "        raise NameError('Unrecognized decay scheduler. Defaulting to cosine.')\n",
    "except NameError:\n",
    "    lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "        initial_learning_rate=INIT_LR,\n",
    "        decay_steps=EPOCHS * trainX.shape[0] // BS)\n",
    "opt=Adam(learning_rate=lr_schedule)\n",
    "autoencoder.compile(loss=LOSS, optimizer=opt)\n",
    "\n",
    "\n",
    "'''\n",
    "ADABOUND\n",
    "'''\n",
    "# WEIGHT_DECAY = 1e-6\n",
    "# regularizer = tf.keras.regularizers.l2(WEIGHT_DECAY / 2)\n",
    "# for layer in autoencoder.layers:\n",
    "#     for attr in ['kernel_regularizer', 'bias_regularizer']:\n",
    "#         if hasattr(layer, attr) and layer.trainable:\n",
    "#             setattr(layer, attr, regularizer)\n",
    "            \n",
    "# autoencoder.compile(optimizer = AdaBound(lr=INIT_LR, final_lr=0.1),\n",
    "#               loss = 'mse')\n",
    "''''''\n",
    "autoencoder.summary()\n",
    "\n",
    "# callbacks=[\n",
    "#     TensorBoard(\n",
    "#     log_dir=args['output_path'], histogram_freq=0, write_graph=True, write_images=False,\n",
    "#     update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "#     embeddings_metadata=None)   \n",
    "# ]\n",
    "def show_predictions(model, inputs, preds):\n",
    "    prediction = model.predict(inputs)\n",
    "    fig, axes = plt.subplots(1, preds, sharex=True, sharey=True, figsize=(15,15))\n",
    "    for i in range(preds):\n",
    "        axes[i].imshow(prediction[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % 50 == 0:\n",
    "            show_predictions(self.model, visImages[:3], 3)\n",
    "            show_predictions(self.model, visImages[3:], 3)\n",
    "            print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "best_loss   = float('inf')\n",
    "best_epoch  = 0\n",
    "class RecordBestLossCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global best_loss\n",
    "        global best_epoch\n",
    "        if logs['val_loss'] < best_loss:\n",
    "            best_epoch = epoch\n",
    "            best_loss  = logs['val_loss']\n",
    "        \n",
    "\n",
    "checkpoint_filepath = './outputs/temp_weights/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "print('[INFO] training autoencoder...')\n",
    "H=autoencoder.fit(\n",
    "    trainData,\n",
    "    validation_data=valData,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[DisplayCallback(), RecordBestLossCallback(), model_checkpoint_callback]\n",
    ")\n",
    "\n",
    "print('[INFO] loading best weights and cleanup')\n",
    "autoencoder.load_weights(checkpoint_filepath)\n",
    "!rm -rf {checkpoint_filepath}/*\n",
    "\n",
    "print('[INFO] Saving model...')\n",
    "autoencoder.save(modelSavePath)\n",
    "\n",
    "print('Final Prediction:')\n",
    "show_predictions(autoencoder, visImages[:3], 3)\n",
    "show_predictions(autoencoder, visImages[3:], 3)\n",
    "\n",
    "print(f'Best val loss: {best_loss} at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(object_path, autoencoder, method=None, model_function=None):\n",
    "    \n",
    "    ground_path  = os.path.join(object_path, 'ground_truth')\n",
    "    images_path  = os.path.join(object_path, 'test')\n",
    "    defect_types = os.listdir(ground_path)\n",
    "\n",
    "    if method == 'uncertainty':\n",
    "        print('[INFO]: Generating uncertainty model')\n",
    "        assert model_function is not None, 'Must provide function that autoencoder was created with if using uncertainty method'\n",
    "        weights = autoencoder.get_weights()\n",
    "        uncertainty_ae = model_function(depth=DEPTH, filters=FILTERS, latentDepth=LATENT, MCDropout=True)\n",
    "        uncertainty_ae.set_weights(weights)\n",
    "        \n",
    "    # Calculate the overall AUC separately. Averaging the AUC's for individual categories gives\n",
    "    # a slightly higher score.\n",
    "    all_ground  = []\n",
    "    all_results = []\n",
    "    for defect_type in defect_types:\n",
    "        ground_vector = []\n",
    "        input_images  = []\n",
    "        masks  = sorted(os.listdir(os.path.join(ground_path, defect_type)))\n",
    "        images = sorted(os.listdir(os.path.join(images_path, defect_type)))\n",
    "        for mask in masks:\n",
    "            full_path = os.path.join(ground_path, defect_type, mask)\n",
    "            ground = cv2.imread(full_path)[...,0]\n",
    "            ground = cv2.resize(ground, (DIM, DIM))\n",
    "            ground[ground > 0] = 1\n",
    "            ground_vector.append(ground)\n",
    "        for image in images:\n",
    "            full_path = os.path.join(images_path, defect_type, image)\n",
    "            image = cv2.imread(full_path)\n",
    "            if DEPTH == 1:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (DIM, DIM)).astype('float32') / 255\n",
    "            input_images.append(image)\n",
    "\n",
    "        # Predict images\n",
    "        input_images = np.array(input_images)\n",
    "        if method =='uncertainty':\n",
    "            results_list = []\n",
    "            # Using 30 runs according to the paper\n",
    "            for i in range(30):\n",
    "                results = uncertainty_ae(input_images).numpy()\n",
    "                results = np.sum(results, axis=-1) / 3\n",
    "                results_list.append(results)\n",
    "            results_vector = np.std(results_list, axis=0)\n",
    "#             plt.imshow(results_vector[0])\n",
    "#             plt.show()\n",
    "            results_vector = results_vector.flatten()\n",
    "        else:\n",
    "            results_vector = autoencoder(input_images).numpy()\n",
    "            if DEPTH == 1:\n",
    "                results_vector = np.squeeze(results_vector, axis=-1)\n",
    "            results_vector = np.abs(results_vector - input_images)\n",
    "            if DEPTH > 1:\n",
    "                results_vector = np.sum(results_vector, axis=-1) / 3\n",
    "            results_vector = results_vector.flatten()\n",
    "        ground_vector = np.array(ground_vector).flatten()\n",
    "        score = roc_auc_score(ground_vector, results_vector)\n",
    "        print(f'AUC for {defect_type}:', score)\n",
    "\n",
    "        for i in ground_vector: all_ground.append(i)\n",
    "        for i in results_vector: all_results.append(i)\n",
    "\n",
    "    full_score = roc_auc_score(all_ground, all_results)\n",
    "    print('Average AUC score:', full_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup cell to save model and show final predictions if manually stop early\n",
    "# show_predictions(autoencoder, visImages[:3], 3)\n",
    "# show_predictions(autoencoder, visImages[3:], 3)\n",
    "autoencoder.save(modelSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup cell to load model if starting new Jupyter kernel\n",
    "autoencoder = tf.keras.models.load_model(modelSavePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_path  = 'data/mvtec_anomaly_detection/carpet/'\n",
    "get_auc(object_path, autoencoder2, method='resid', model_function=create_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_adabound import AdaBound\n",
    "INIT_LR = 1e-4\n",
    "# print('[INFO] making trainable edition of autoencoder...')\n",
    "# weights = autoencoder.get_weights()\n",
    "# autoencoder2 = create_autoencoder(depth=DEPTH, filters=FILTERS, latentDepth=LATENT, trainable=True, MCDropout=False)\n",
    "# autoencoder2.set_weights(weights)\n",
    "# lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "#     initial_learning_rate=INIT_LR,\n",
    "#     decay_steps=EPOCHS * trainX.shape[0] // BS)\n",
    "# opt=Adam(learning_rate=lr_schedule)\n",
    "# autoencoder2.compile(loss=LOSS, optimizer=opt)\n",
    "\n",
    "\n",
    "'''\n",
    "ADABOUND\n",
    "'''\n",
    "# WEIGHT_DECAY = 1e-6\n",
    "# regularizer = tf.keras.regularizers.l2(WEIGHT_DECAY / 2)\n",
    "# for layer in autoencoder.layers:\n",
    "#     for attr in ['kernel_regularizer', 'bias_regularizer']:\n",
    "#         if hasattr(layer, attr) and layer.trainable:\n",
    "#             setattr(layer, attr, regularizer)\n",
    "            \n",
    "# autoencoder.compile(optimizer = AdaBound(lr=INIT_LR, final_lr=0.1),\n",
    "#               loss = 'mse')\n",
    "''''''\n",
    "autoencoder2.summary()\n",
    "\n",
    "# callbacks=[\n",
    "#     TensorBoard(\n",
    "#     log_dir=args['output_path'], histogram_freq=0, write_graph=True, write_images=False,\n",
    "#     update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "#     embeddings_metadata=None)   \n",
    "# ]\n",
    "def show_predictions(model, inputs, preds):\n",
    "    prediction = model.predict(inputs)\n",
    "    fig, axes = plt.subplots(1, preds, sharex=True, sharey=True, figsize=(15,15))\n",
    "    for i in range(preds):\n",
    "        axes[i].imshow(prediction[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % 50 == 0:\n",
    "            show_predictions(self.model, visImages[:3], 3)\n",
    "            show_predictions(self.model, visImages[3:], 3)\n",
    "            print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "best_loss   = float('inf')\n",
    "best_epoch  = 0\n",
    "class RecordBestLossCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global best_loss\n",
    "        global best_epoch\n",
    "        if logs['val_loss'] < best_loss:\n",
    "            best_epoch = epoch\n",
    "            best_loss  = logs['val_loss']\n",
    "        \n",
    "\n",
    "checkpoint_filepath = './outputs/temp_weights/'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "print('[INFO] training autoencoder...')\n",
    "H=autoencoder2.fit(\n",
    "    trainData,\n",
    "    validation_data=valData,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[DisplayCallback(), RecordBestLossCallback(), model_checkpoint_callback]\n",
    ")\n",
    "\n",
    "print('[INFO] loading best weights and cleanup')\n",
    "autoencoder2.load_weights(checkpoint_filepath)\n",
    "!rm -rf {checkpoint_filepath}/*\n",
    "\n",
    "print('[INFO] Saving model...')\n",
    "autoencoder.save(modelSavePath2)\n",
    "\n",
    "print('Final Prediction:')\n",
    "show_predictions(autoencoder2, visImages[:3], 3)\n",
    "show_predictions(autoencoder2, visImages[3:], 3)\n",
    "\n",
    "print(f'Best val loss: {best_loss} at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfer-learning-autoencoders",
   "language": "python",
   "name": "transfer-learning-autoencoders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
