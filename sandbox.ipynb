{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, LeakyReLU, Activation,\n",
    "                                    Flatten, Dense, Reshape, Dropout, Add, Input)\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files\n",
    "import random\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEUnet:\n",
    "    @staticmethod\n",
    "    def collin(dim=1024,depth=1,filters=(16,32,64,128,256,512), latentDepth=1024):\n",
    "        '''\n",
    "        dim: dim of square input image\n",
    "        depth: number of channels in input image\n",
    "        filters: tuple for set of convolution filters, defaulted to paper implementation\n",
    "        latentDepth: depth of latent layer\n",
    "        '''\n",
    "        inputShape=(dim,dim,depth)\n",
    "        chanDim=-1 #channel dimension (-1) applies batch norm for each layer (or depth) Unknowns=[mu(untrainable),sigma(untrainable),gamma,beta]*depth\n",
    "\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "\n",
    "        # generate:conv=>relu=>bn layers\n",
    "        # additionally, save skip connections and add dropout\n",
    "        # Dropout layers increase by 0.1 as the encoder gets deeper\n",
    "        skips = []\n",
    "        for i, f in enumerate(filters):\n",
    "            x = Conv2D(f,(5,5),strides=2,padding='same')(x)\n",
    "            x1 = [x]\n",
    "            skips.append(x1[0])\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x) \n",
    "            x = Dropout(0.1 * i)(x)\n",
    "\n",
    "        # For the UNet build, use a conv layer instead of dense for latent\n",
    "        latent = Conv2D(latentDepth, (5, 5), strides=2, padding='same', name='latent')(x)\n",
    "        y = latent\n",
    "        \n",
    "        #decode the latent space, incorporating skip layers\n",
    "        for i, f in reversed(list(enumerate(filters))):\n",
    "            y = Conv2DTranspose(f,(5,5),strides=2,padding='same')(y)\n",
    "            y = Add()([skips[i], y])\n",
    "            y = LeakyReLU(alpha=0.2)(y)\n",
    "            y = BatchNormalization(axis=chanDim)(y)\n",
    "\n",
    "        \n",
    "        # Technically not strictly following the paper, this should be a straight-up upsampling\n",
    "        y = Conv2DTranspose(depth,(5,5),strides=2,padding='same')(y)\n",
    "        outputs = Activation('sigmoid')(y)\n",
    "        autoencoder = Model(inputs,outputs,name='autoencoder')\n",
    "\n",
    "        return autoencoder\n",
    "\n",
    "    def mobilenet(output_channels, input_dim):\n",
    "\n",
    "        base_model = tf.keras.applications.MobileNetV2(input_shape=[input_dim, input_dim, 3], include_top=False)\n",
    "\n",
    "        # Use the activations of these layers\n",
    "        layer_names = [\n",
    "            'block_1_expand_relu',   # 64x64\n",
    "            'block_3_expand_relu',   # 32x32\n",
    "            'block_6_expand_relu',   # 16x16\n",
    "            'block_13_expand_relu',  # 8x8\n",
    "            'block_16_project',      # 4x4\n",
    "        ]\n",
    "        layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "        # Create the feature extraction model\n",
    "        down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "        down_stack.trainable = True # From TLP experience, this should be True\n",
    "\n",
    "        # \"Add\" layer version\n",
    "        up_stack = [\n",
    "            pix2pix.upsample(576, 3),  # 4x4 -> 8x8\n",
    "            pix2pix.upsample(192, 3),  # 8x8 -> 16x16\n",
    "            pix2pix.upsample(144, 3),  # 16x16 -> 32x32\n",
    "            pix2pix.upsample(96, 3),   # 32x32 -> 64x64\n",
    "        ]\n",
    "\n",
    "#         # \"Concat\" layer version\n",
    "#         up_stack = [\n",
    "#             pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "#             pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "#             pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "#             pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "#         ]\n",
    "\n",
    "        inputs = tf.keras.layers.Input(shape=[input_dim, input_dim, 3])\n",
    "        x = inputs\n",
    "\n",
    "        # Downsampling through the model\n",
    "        skips = down_stack(x)\n",
    "        x = skips[-1]\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            # swap concat for add\n",
    "            add = tf.keras.layers.Add()\n",
    "            x = add([x, skip])\n",
    "\n",
    "        # This is the last layer of the model\n",
    "        last = tf.keras.layers.Conv2DTranspose(\n",
    "            output_channels, 3, strides=2,\n",
    "            padding='same')(x)  #64x64 -> 128x128\n",
    "\n",
    "        # Modification to make autoencoder\n",
    "        outputs = Activation('sigmoid')(last)\n",
    "        autoencoder = Model(inputs,outputs,name='autoencoder')\n",
    "\n",
    "        return autoencoder\n",
    "    \n",
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 1000\n",
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "LOSS = 'mse'\n",
    "testImg = 'data/mvtec_anomaly_detection/carpet/test/hole/000.png'\n",
    "modelSavePath = 'outputs/prototype.pb'\n",
    "historySavePath = 'outputs/prototype_history'\n",
    "def create_autoencoder():\n",
    "    return AEUnet.mobilenet(3, DIM)\n",
    "STAIN_PROPS = [10, 40, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as Prototype but for only 100 epochs, as 50 was where things were really good in the last\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "LOSS = 'mse'\n",
    "testImg = 'data/mvtec_anomaly_detection/carpet/test/hole/000.png'\n",
    "modelSavePath = 'outputs/prototype1.pb'\n",
    "historySavePath = 'outputs/prototype1_history'\n",
    "def create_autoencoder():\n",
    "    return AEUnet.mobilenet(3, DIM)\n",
    "STAIN_PROPS = [10, 40, 0, 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as Prototype but for only 100 epochs, and less aggro stain colors\n",
    "DIM = 256\n",
    "DATA_DIR = 'data/mvtec_anomaly_detection/carpet/train/good/'\n",
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "BS = 8\n",
    "LOSS = 'mse'\n",
    "testImg = 'data/mvtec_anomaly_detection/carpet/test/hole/000.png'\n",
    "modelSavePath = 'outputs/prototype2.pb'\n",
    "historySavePath = 'outputs/prototype1_history'\n",
    "def create_autoencoder():\n",
    "    return AEUnet.mobilenet(3, DIM)\n",
    "STAIN_PROPS = [10, 40, 150, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AEUnet.mobilenet(3, DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint, uniform\n",
    "from numpy.random import normal, uniform\n",
    "from skimage.util import random_noise\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.draw import ellipse_perimeter, disk\n",
    "from scipy.interpolate import interp1d \n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def add_stain(img,\n",
    "              min_size=10,\n",
    "              max_size=40,\n",
    "              min_color=0.,\n",
    "              max_color=255.,\n",
    "              cx_range=None,\n",
    "              cy_range=None,\n",
    "              irregularity=0.08,\n",
    "              blur=0.08):\n",
    "    '''\n",
    "    Draw an ellipse-like shape \n",
    "    INPUT : \n",
    "        - img: image to corrupt with an elliptical stain\n",
    "        - min_size, max_size: bounds for stain size in percentage\n",
    "        - min_color, max_color: bounds of intensities to sample from (0 - 255)\n",
    "        - cx_range, cy_range: (optional) tuples to constrain location (in percentage of image) of stain center\n",
    "        - irregularity: level of irregularity of the ellipse (0: no)\n",
    "        - blur: blur edges (0: no)\n",
    "    OUTPUT: \n",
    "        - corrupted image\n",
    "    '''\n",
    "    \n",
    "    assert min_size <= max_size, 'min_size must not be greater than max_size'\n",
    "    assert min_size >= 0 and max_size <= 100, 'invalid sizes. Must be in range [0, 100]'\n",
    "    assert min_color <= max_color, 'min_color must not be greater than max_color'\n",
    "    assert max_color >= 0 and max_color <= 255, 'invalid colors, Must be in range[0, 255]'\n",
    "\n",
    "    color    = randint(min_color, max_color)\n",
    "    col, row = img.shape[1], img.shape[0]\n",
    "    rotation = uniform(0, 2*np.pi)\n",
    "    ax_x = int(randint(min_size, max_size)/2 / 100*col)\n",
    "    ax_y = int(randint(min_size, max_size)/2 / 100*row)\n",
    "    \n",
    "    if cx_range is not None:\n",
    "        cx_lower = int(cx_range[0] / 100.*col)\n",
    "        cx_upper = int(cx_range[1] / 100.*col)\n",
    "    else:\n",
    "        cx_lower = int(max_size/2 / 100.*col)\n",
    "        cx_upper = int((100 - max_size/2) / 100.*col)\n",
    "        \n",
    "    if cy_range is not None:\n",
    "        cy_lower = int(cy_range[0] / 100.*row)\n",
    "        cy_upper = int(cy_range[1] / 100.*row)\n",
    "    else:\n",
    "        cy_lower = int(max_size/2 / 100.*row)\n",
    "        cy_upper = int((100 - max_size/2) / 100.*row)\n",
    "    \n",
    "    cx, cy = randint(cx_lower, cx_upper), randint(cy_lower, cy_upper)\n",
    "    y,x      = ellipse_perimeter(cy, cx, ax_y, ax_x, rotation)\n",
    "    # Flip x and y because opencv is annoying\n",
    "    contour  = np.array([[i,j] for i,j in zip(x,y)])\n",
    "    # Change the shape of the ellipse \n",
    "    if irregularity > 0: \n",
    "        contour = _perturbate_ellipse(contour, cx, cy, (ax_x+ax_y)/2, irregularity)\n",
    "\n",
    "    mask = np.zeros((row, col)) \n",
    "    mask = cv2.drawContours(mask, [contour], -1, 1, -1)\n",
    "\n",
    "    if blur != 0 : \n",
    "        mask = gaussian_filter(mask, max(ax_x,ax_y)*blur)\n",
    "\n",
    "    rgb_mask     = np.dstack([mask]*3)\n",
    "    not_modified = np.subtract(np.ones(img.shape), rgb_mask)\n",
    "    stain        = 255*random_noise(np.zeros(img.shape), mode='gaussian', mean = color/255., var = 0.05/255.)\n",
    "    result       = np.add( np.multiply(img,not_modified), np.multiply(stain,rgb_mask) ) \n",
    "\n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "'''\n",
    "Helper functions for stain\n",
    "'''\n",
    "def _perturbate_ellipse(contour, cx, cy, diag, irregularity):\n",
    "    # Keep only some points\n",
    "    if len(contour) < 20: \n",
    "        pts = contour\n",
    "    else: \n",
    "        pts = contour[0::int(len(contour)/20)]\n",
    "\n",
    "    # Perturbate coordinates\n",
    "    for idx,pt in enumerate(pts): \n",
    "        pts[idx] = [pt[0]+randint(-int(diag*irregularity), int(diag*irregularity)),\n",
    "                    pt[1]+randint(-int(diag*irregularity),int(diag*irregularity))]\n",
    "    pts = sorted(pts, key=lambda p: _clockwiseangle(p, cx, cy))\n",
    "    pts.append([pts[0][0], pts[0][1]])\n",
    "\n",
    "    # Interpolate between remaining points\n",
    "    i = np.arange(len(pts))\n",
    "    interp_i = np.linspace(0, i.max(), 10 * i.max())\n",
    "    yi = interp1d(i, np.array(pts)[:,0], kind='cubic')(interp_i)\n",
    "    xi = interp1d(i, np.array(pts)[:,1], kind='cubic')(interp_i) \n",
    " \n",
    "    return np.array([[int(i),int(j)] for i,j in zip(yi,xi)])\n",
    "\n",
    "def _clockwiseangle(point, cx, cy):\n",
    "    refvec = [0 , 1]\n",
    "    vector = [point[0]-cy, point[1]-cx]\n",
    "    norm   = math.hypot(vector[0], vector[1])\n",
    "    # If length is zero there is no angle\n",
    "    if norm == 0:\n",
    "        return -math.pi\n",
    "    normalized = [vector[0]/norm, vector[1]/norm]\n",
    "    dotprod    = normalized[0]*refvec[0] + normalized[1]*refvec[1] \n",
    "    diffprod   = refvec[1]*normalized[0] - refvec[0]*normalized[1] \n",
    "    angle      = math.atan2(diffprod, dotprod)\n",
    "    if angle < 0:\n",
    "        return 2*math.pi+angle\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stain_wrapper(image, target):\n",
    "    image = image.numpy()\n",
    "    image = add_stain(image, STAIN_PROPS[0], STAIN_PROPS[1], STAIN_PROPS[2], STAIN_PROPS[3])\n",
    "    return image.astype('float32') / 255, target\n",
    "\n",
    "stain_lambda = lambda x, y: tf.py_function(stain_wrapper, [x, y], ['float32', 'float32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data input pipeline\n",
    "imgDir = os.listdir(DATA_DIR)\n",
    "# Note: not grayscale!!!\n",
    "\n",
    "imgs = np.empty((len(imgDir), DIM, DIM, 3), dtype='uint8')\n",
    "for i, imgName in enumerate(imgDir):\n",
    "    img = cv2.imread(os.path.join(DATA_DIR, imgName))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    imgs[i] = img\n",
    "\n",
    "(trainX, valX)=train_test_split(imgs,test_size=0.2,random_state=42)\n",
    "\n",
    "trainData = tf.data.Dataset.from_tensor_slices((trainX, trainX.astype('float32') / 255))\\\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
    "            .map(stain_lambda)\\\n",
    "            .shuffle(trainX.shape[0])\\\n",
    "            .batch(BS)\n",
    "            \n",
    "valData   = tf.data.Dataset.from_tensor_slices((valX, valX.astype('float32') / 255))\\\n",
    "            .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
    "            .map(stain_lambda)\\\n",
    "            .batch(BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "for i, data in enumerate(trainData):\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.imshow(np.hstack((data[0][0], data[1][0])))\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a few for testing\n",
    "visImages = np.empty((3, DIM, DIM, 3))\n",
    "visImages[0] = np.expand_dims(add_stain(trainX[0]).astype('float32') / 255, axis=0)\n",
    "visImages[1] = np.expand_dims(valX[0], axis=0).astype('float32') / 255\n",
    "test = cv2.imread(testImg)\n",
    "test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n",
    "test = cv2.resize(test, (256, 256))\n",
    "visImages[2] = np.expand_dims(test, axis=(0)).astype('float32') / 255\n",
    "plt.imshow(visImages[0])\n",
    "plt.show()\n",
    "plt.imshow(visImages[1])\n",
    "plt.show()\n",
    "plt.imshow(visImages[2])\n",
    "plt.show()\n",
    "visData = tf.data.Dataset.from_tensor_slices(visImages).batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras_adabound import AdaBound\n",
    "\n",
    "print('[INFO] building autoencoder...')\n",
    "K.clear_session()\n",
    "# autoencoder=TutorialUnet(3)\n",
    "autoencoder = create_autoencoder()  \n",
    "lr_schedule = tf.keras.experimental.CosineDecay(\n",
    "    initial_learning_rate=INIT_LR,\n",
    "    decay_steps=EPOCHS * trainX.shape[0] // BS)\n",
    "opt=Adam(learning_rate=lr_schedule)\n",
    "autoencoder.compile(loss=LOSS, optimizer=opt)\n",
    "\n",
    "\n",
    "'''\n",
    "ADABOUND\n",
    "'''\n",
    "# WEIGHT_DECAY = 1e-6\n",
    "# regularizer = tf.keras.regularizers.l2(WEIGHT_DECAY / 2)\n",
    "# for layer in autoencoder.layers:\n",
    "#     for attr in ['kernel_regularizer', 'bias_regularizer']:\n",
    "#         if hasattr(layer, attr) and layer.trainable:\n",
    "#             setattr(layer, attr, regularizer)\n",
    "            \n",
    "# autoencoder.compile(optimizer = AdaBound(lr=INIT_LR, final_lr=0.1),\n",
    "#               loss = 'mse')\n",
    "''''''\n",
    "autoencoder.summary()\n",
    "\n",
    "# callbacks=[\n",
    "#     TensorBoard(\n",
    "#     log_dir=args['output_path'], histogram_freq=0, write_graph=True, write_images=False,\n",
    "#     update_freq='epoch', profile_batch=2, embeddings_freq=0,\n",
    "#     embeddings_metadata=None)   \n",
    "# ]\n",
    "def show_predictions(model, inputs, preds):\n",
    "    prediction = model.predict(inputs)\n",
    "    fig, axes = plt.subplots(1, preds, sharex=True, sharey=True, figsize=(15,15))\n",
    "    for i in range(preds):\n",
    "        axes[i].imshow(prediction[i], cmap=plt.get_cmap('Greys_r'))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch % 50 == 0:\n",
    "            show_predictions(self.model, visImages, 3)\n",
    "            print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n",
    "\n",
    "print('[INFO] training autoencoder...')\n",
    "H=autoencoder.fit(\n",
    "    trainData,\n",
    "    validation_data=valData,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[DisplayCallback()]\n",
    ")\n",
    "\n",
    "print('Saving...')\n",
    "autoencoder.save(modelSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'])\n",
    "with open(historySavePath, 'wb') as file_pi:\n",
    "    pickle.dump(H.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "autoencoder = tf.keras.models.load_model(modelSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on val images\n",
    "for batch in valData:\n",
    "    img = batch[0][:1]\n",
    "    target = batch[1][0]\n",
    "    pred = autoencoder.predict(img)[0]\n",
    "    \n",
    "    resid = np.abs(pred - img[0])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(15,15))\n",
    "    axes[0].imshow(img[0])\n",
    "    axes[1].imshow(pred)\n",
    "    axes[2].imshow(resid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on test images\n",
    "TEST_DIR = 'data/mvtec_anomaly_detection/carpet/test/cut/'\n",
    "testImgs = os.listdir(TEST_DIR)\n",
    "clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8,8))\n",
    "\n",
    "for imgName in testImgs:\n",
    "    img = cv2.imread(os.path.join(TEST_DIR, imgName))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (DIM, DIM)).astype('float32') / 255\n",
    "    \n",
    "    pred = autoencoder.predict(np.expand_dims(img, axis=0))[0]\n",
    "    \n",
    "    resid = (np.abs(pred - img) * 255).astype('uint8')\n",
    "    for channel in range(3):\n",
    "        resid[..., channel] = clahe.apply(resid[..., channel])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(15,15))\n",
    "    axes[0].imshow(img)\n",
    "    axes[1].imshow(pred)\n",
    "    axes[2].imshow(resid)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG\n",
    "\n",
    "- __v0__ Interestingly, gets really good at epoch 50 then starts to just straight up reproduce errors. Maybe just trained for too long? Or need to be more aggressive with staining? TLoss to 2.2136e-04, VLoss to 2.8629e-04.\n",
    "    - Hm, looking at the val data, it looks like it's just putting the same pattern under every stain. Not necessarily bad lol it is a uniform pattern after all. But could it just be learning to plug in something generic and that's why when we feed it an actual hole it just spits it out. First thing to try is stopping the training at 50 epochs. Next is to actually be LESS aggressive with staining size.\n",
    "    - HOWEVER, barring a few cases, the residual actually grabs the defective area pretty well.\n",
    "- __v1__ Hm, the coloration is off. TL to 5.1752e-04, VL to 0.0033. Trained for 100 because learning rate decay. Let's try being less aggro with staining. Size is probably ok, but limit the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfer-learning-autoencoders",
   "language": "python",
   "name": "transfer-learning-autoencoders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
